{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiomics GRN inference evaluation\n",
    "## VAE-SEM model\n",
    "### by Jalil Nourisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anndata\n",
      "  Using cached anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
      "  Using cached array_api_compat-1.8-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: exceptiongroup in /home/jnourisa/miniconda3/envs/pytorch/lib/python3.9/site-packages (from anndata) (1.2.2)\n",
      "Collecting h5py>=3.1 (from anndata)\n",
      "  Using cached h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting natsort (from anndata)\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/jnourisa/miniconda3/envs/pytorch/lib/python3.9/site-packages (from anndata) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jnourisa/miniconda3/envs/pytorch/lib/python3.9/site-packages (from anndata) (24.1)\n",
      "Collecting pandas!=2.1.0rc0,!=2.1.2,>=1.4 (from anndata)\n",
      "  Using cached pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting scipy>1.8 (from anndata)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jnourisa/miniconda3/envs/pytorch/lib/python3.9/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/jnourisa/miniconda3/envs/pytorch/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
      "Using cached anndata-0.10.9-py3-none-any.whl (128 kB)\n",
      "Using cached array_api_compat-1.8-py3-none-any.whl (38 kB)\n",
      "Using cached h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Using cached pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Using cached natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, array-api-compat, tzdata, scipy, natsort, h5py, pandas, anndata\n",
      "Successfully installed anndata-0.10.9 array-api-compat-1.8 h5py-3.11.0 natsort-8.4.0 pandas-2.2.2 pytz-2024.2 scipy-1.13.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:10:17.111822Z",
     "iopub.status.busy": "2024-06-27T16:10:17.111469Z",
     "iopub.status.idle": "2024-06-27T16:10:19.741614Z",
     "shell.execute_reply": "2024-06-27T16:10:19.740860Z",
     "shell.execute_reply.started": "2024-06-27T16:10:17.111796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "-----Seed Set!-----\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# from sklearn.metrics import r2_score, roc_auc_score\n",
    "# from sklearn.metrics import mean_squared_error \n",
    "# from sklearn.decomposition import TruncatedSVD, KernelPCA    \n",
    "# from sklearn.ensemble import  RandomForestRegressor\n",
    "import os\n",
    "import random \n",
    "# import category_encoders \n",
    "import torch.nn as nn \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "SEED = 0xCAFE\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('using device: cuda')\n",
    "else:\n",
    "    print('using device: cpu')\n",
    "    USE_GPU = False\n",
    "\n",
    "def seed_everything():\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print('-----Seed Set!-----')\n",
    "seed_everything()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_rna = ad.read_h5ad('../output/scRNA/adata_rna.h5ad')\n",
    "gene_names = adata_rna.var_names\n",
    "gene_name_dict = {gene_name: i for i, gene_name in enumerate(gene_names)}\n",
    "n_genes = len(gene_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process grn net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download GRN\n",
    "# # net = pd.read_csv(\"https://github.com/pablormier/omnipath-static/raw/main/op/collectri-26.09.2023.zip\")\n",
    "# net = pd.read_csv(\"../output/benchmark/grn_models/scenicplus.csv\")\n",
    "\n",
    "# # Iterate over the edges (regulatory relationships)\n",
    "# edge_idx = set()\n",
    "# grn_weights = []\n",
    "# for gene_a, gene_b, weight in zip(net['source'], net['target'], net['weight']):\n",
    "#     if (gene_a not in gene_name_dict) or (gene_b not in gene_name_dict):\n",
    "#         continue  # Consider only gene names that are present in the training data\n",
    "#     i = gene_name_dict[gene_a]  # Index of first gene\n",
    "#     j = gene_name_dict[gene_b]  # Index of second gene\n",
    "#     grn_weights.append(weight)\n",
    "#     edge_idx.add((i, j))\n",
    "# edge_idx = np.asarray(list(edge_idx), dtype=int)\n",
    "\n",
    "# # Convert list of edges into an adjacency matrix\n",
    "# grn = np.zeros((len(gene_names), len(gene_names)))\n",
    "# for i, weight in enumerate(grn_weights):\n",
    "#     weight = 1 if weight>=0 else -1\n",
    "#     grn[edge_idx[i, 0], edge_idx[i, 1]] = weight\n",
    "\n",
    "# # Remove rows and columns with no annotation\n",
    "# grn_mask = np.logical_or(grn.sum(axis=0) > 0, grn.sum(axis=1) > 0)\n",
    "# grn = grn[grn_mask, :][:, grn_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00639388880957349"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grn = pd.read_csv(\"../output/benchmark/grn_models/scenicplus.csv\", index_col=0)\n",
    "grn = pd.read_csv(\"https://github.com/pablormier/omnipath-static/raw/main/op/collectri-26.09.2023.zip\")\n",
    "# only keep those with both tf and gene included in gene_names\n",
    "grn = grn[grn.source.isin(gene_names) & grn.target.isin(gene_names)].reset_index(drop=True)\n",
    "# binarize\n",
    "weights = [1 if weight>=0 else -1 for weight in grn.weight  ] \n",
    "grn.weight = weights\n",
    "\n",
    "GT = grn.pivot(index='source', columns='target', values='weight').fillna(0)\n",
    " \n",
    "GT = GT.abs() # binary \n",
    "GT.abs().mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:43:43.020607Z",
     "iopub.status.busy": "2024-06-27T16:43:43.020220Z",
     "iopub.status.idle": "2024-06-27T16:43:43.023916Z",
     "shell.execute_reply": "2024-06-27T16:43:43.023278Z",
     "shell.execute_reply.started": "2024-06-27T16:43:43.020582Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 2.],\n",
       "        [0., 1., 2., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 3., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = adata_rna.X.todense() #TODO: modify the train code to deal with sparse\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:28:37.686198Z",
     "iopub.status.busy": "2024-06-27T16:28:37.685772Z",
     "iopub.status.idle": "2024-06-27T16:28:39.747151Z",
     "shell.execute_reply": "2024-06-27T16:28:39.746275Z",
     "shell.execute_reply.started": "2024-06-27T16:28:37.686168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features \n",
    "        self.labels = labels \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "features = torch.tensor(Y, dtype=torch.float32) \n",
    "labels = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "dataset = Dataset(features=features, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:35:14.952828Z",
     "iopub.status.busy": "2024-06-27T16:35:14.952450Z",
     "iopub.status.idle": "2024-06-27T16:35:14.956529Z",
     "shell.execute_reply": "2024-06-27T16:35:14.955864Z",
     "shell.execute_reply.started": "2024-06-27T16:35:14.952800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MultiOutputTargetEncoder:\n",
    "#     def __init__(self):\n",
    "#         self.encoders: List[category_encoders.leave_one_out.LeaveOneOutEncoder] = []\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def new_encoder() -> category_encoders.leave_one_out.LeaveOneOutEncoder:\n",
    "#         return category_encoders.leave_one_out.LeaveOneOutEncoder(return_df=False)\n",
    "    \n",
    "#     def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "#         self.encoders = []\n",
    "#         for j in tqdm.tqdm(range(y.shape[1]), desc='fit LOO encoders'):\n",
    "#             self.encoders.append(MultiOutputTargetEncoder.new_encoder())\n",
    "#             self.encoders[-1].fit(X, y[:, j])\n",
    "    \n",
    "#     def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "#         Z = []\n",
    "#         for encoder in tqdm.tqdm(self.encoders, desc='transform LOO encoders'):\n",
    "#             y_hat = encoder.transform(X)\n",
    "#             Z.append(y_hat)\n",
    "#         Z = np.asarray(Z)\n",
    "#         return np.transpose(Z, (1, 0, 2))\n",
    "# encoder = MultiOutputTargetEncoder()\n",
    "\n",
    "# encoder.fit(np.asarray([df_train.index.get_level_values(var) for var in features_X]).T, df_train.values)\n",
    "\n",
    "# X = encoder.transform(np.asarray([df_train.index.get_level_values(var) for var in features_X]).T)\n",
    "# X_submit = encoder.transform(np.asarray([df_test.index.get_level_values(var) for var in features_X]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:44:18.031771Z",
     "iopub.status.busy": "2024-06-27T16:44:18.031386Z",
     "iopub.status.idle": "2024-06-27T16:44:18.037461Z",
     "shell.execute_reply": "2024-06-27T16:44:18.036630Z",
     "shell.execute_reply.started": "2024-06-27T16:44:18.031745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NN_AE(torch.nn.Module): \n",
    "    '''\n",
    "    Simple AE design.\n",
    "    '''\n",
    "    def __init__(self, n_genes:int,n_nodes_hidden:int=120):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.n_genes = n_genes\n",
    "        dropout_rate = .1\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(n_genes, n_nodes_hidden),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(dropout_rate),  \n",
    "                \n",
    "                nn.Linear(n_nodes_hidden, 64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(dropout_rate),  \n",
    "            \n",
    "                nn.Linear(64, 120),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(dropout_rate),  \n",
    "            \n",
    "                nn.Linear(n_nodes_hidden, n_genes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        return x, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T15:18:39.603992Z",
     "iopub.status.busy": "2024-06-03T15:18:39.603535Z",
     "iopub.status.idle": "2024-06-03T15:18:39.613692Z",
     "shell.execute_reply": "2024-06-03T15:18:39.612813Z",
     "shell.execute_reply.started": "2024-06-03T15:18:39.603952Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def background_noise( #we use dropouts instead of this for pseudobulked data\n",
    "#     *size: int,\n",
    "#     cutoff: float = 0.05,\n",
    "#     device: str = 'cuda',\n",
    "#     generator: torch.Generator = None) -> torch.Tensor:\n",
    "#     sign = 2 * torch.randint(0, 2, size, device=device) - 1\n",
    "\n",
    "#     return sign * torch.log10(cutoff +  torch.rand(*size, generator=generator, device=device) * (1. - cutoff))\n",
    "# class Scaler(torch.nn.Module): #from antoine\n",
    "    \n",
    "#     def __init__(self, m: int) -> None:\n",
    "#         torch.nn.Module.__init__(self)\n",
    "#         self.m: int = m\n",
    "#         self.a: torch.Tensor = torch.nn.Parameter(torch.ones((1, self.m)))\n",
    "#         self.b: torch.Tensor = torch.nn.Parameter(torch.zeros((1, self.m)))\n",
    "    \n",
    "#     def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "#         return self.a * X + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:44:21.565567Z",
     "iopub.status.busy": "2024-06-27T16:44:21.565166Z",
     "iopub.status.idle": "2024-06-27T16:44:21.700323Z",
     "shell.execute_reply": "2024-06-27T16:44:21.699132Z",
     "shell.execute_reply.started": "2024-06-27T16:44:21.565537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.425, R2:0.574: 100%|██████████| 10/10 [01:06<00:00,  6.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def train_nn(model, dataloader, n_epoch, loss_func):\n",
    "    model = model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-8)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, \n",
    "                patience=5, threshold_mode='rel', threshold=0.0001, cooldown=5, min_lr=1e-10, eps=1e-8)\n",
    "\n",
    "    pbar = tqdm.tqdm(range(n_epoch))\n",
    "    for i_epoch in pbar:\n",
    "        rel_loss_store = []\n",
    "        Y_pred_stack = []\n",
    "        Y_true_stack = []\n",
    "        for batch_idx, (feature_batch, label_batch) in enumerate(dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data_batch = feature_batch.to('cuda') #TODO: this should be done once \n",
    "            label_batch = label_batch.to('cuda')\n",
    "            \n",
    "            if False: # add background noise\n",
    "                generator = torch.Generator(device='cuda').manual_seed(32)\n",
    "                label_batch = label_batch + 0.2 * background_noise(*label_batch.size(), generator=generator) #TODO: optimize the weight\n",
    "\n",
    "            x_pred, mu, log_var = model(data_batch) #forward\n",
    "\n",
    "            Y_true_stack.append(label_batch.cpu().detach().numpy())\n",
    "            Y_pred_stack.append(x_pred.cpu().detach().numpy())\n",
    "\n",
    "            loss_x = loss_func(x_pred, label_batch)\n",
    "\n",
    "            #loss_KL =  - 0.5 * torch.sum(1.0 + log_var - mu.pow(2) - log_var.exp()) #TODO: fix this\n",
    "\n",
    "            beta = 1\n",
    "            #loss = loss_x + beta*loss_KL\n",
    "            loss = loss_x \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # baseline pred\n",
    "            Y_pred_mean = torch.mean(label_batch, axis=0)\n",
    "            loss_baseline = loss_func(label_batch, Y_pred_mean)\n",
    "            rel_loss = loss_x/loss_baseline\n",
    "            rel_loss_store.append(rel_loss.item())\n",
    "        # if i_epoch%10==0:\n",
    "        #     # AUROC\n",
    "        #     mask = ~np.eye(n_genes, dtype=bool)\n",
    "        #     grn_pred = np.abs(model.A.cpu().data.numpy())\n",
    "        #     print('AUROC', roc_auc_score(np.abs(grn_net[mask]), grn_pred[mask]))\n",
    "\n",
    "        mean_rel_loss = np.mean(rel_loss_store)\n",
    "        scheduler.step(mean_rel_loss)\n",
    "\n",
    "        y_pred = np.concatenate(Y_pred_stack, axis=0)\n",
    "        y_true = np.concatenate(Y_true_stack, axis=0)\n",
    "\n",
    "        r2 = r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
    "\n",
    "        pbar.set_description(f'Rel loss: {mean_rel_loss:.3f}, R2:{r2:.3f}')\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "batch_size = int(Y.shape[0]/20)\n",
    "n_epoch = 10\n",
    "n_model = 1\n",
    "num_workers = 4\n",
    "shuffle_data = True\n",
    "loss_func = lambda y_pred, y_true: torch.sum(torch.square(y_pred-y_true))\n",
    "\n",
    "models = []\n",
    "# y_pred_submit_all = []\n",
    "for i in range(n_model):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_data, num_workers=num_workers)\n",
    "    model = NN_AE(n_genes)\n",
    "    model = train_nn(model, dataloader, n_epoch, loss_func)\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to capture activations for all layers\n",
    "nodel = model.to('cpu')\n",
    "model.eval()\n",
    "layers = model.mlp\n",
    "\n",
    "activations = []\n",
    "\n",
    "def get_activation_hook(layer_index):\n",
    "    def hook(module, input, output):\n",
    "        activations[layer_index] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialize activations list\n",
    "activations = [None] * len(layers)\n",
    "\n",
    "# Register hooks for all Linear layers\n",
    "for idx, layer in enumerate(model.mlp):\n",
    "    layer.register_forward_hook(get_activation_hook(idx))\n",
    "\n",
    "# Forward pass to get activations\n",
    "with torch.no_grad():\n",
    "    _ = model(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-14.6386, -11.2354,  -2.3267,   3.0302,  -0.5415])\n",
      "tensor([-2.9277, -2.2471, -0.4653,  3.0302, -0.1083])\n",
      "tensor([-2.9277, -2.2471, -0.4653,  3.0302, -0.1083])\n"
     ]
    }
   ],
   "source": [
    "print(activations[0][0][0:5])\n",
    "print(activations[1][0][0:5])\n",
    "print(activations[2][0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22778, 120)\n",
      "(120, 64)\n",
      "(64, 120)\n",
      "(120, 22778)\n"
     ]
    }
   ],
   "source": [
    "# adjusted the weights based on the activaition degrees (due to activaition function)\n",
    "weights_adj_all = []\n",
    "n_layers = len(model.mlp)\n",
    "for i in range(n_layers):\n",
    "    layer = model.mlp[i]\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        weight = layer.weight.detach().numpy().T\n",
    "        if i < (n_layers-1):\n",
    "            # weights_adj =  weight \n",
    "            activation_per_node = activations[i+1].mean(axis=0, keepdims=True).numpy() #activation function is always one ahead\n",
    "            weights_adj =  weight * activation_per_node\n",
    "        else:\n",
    "            weights_adj = weight\n",
    "        weights_adj_all.append(weights_adj)\n",
    "        print(weights_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate regulatory interactions\n",
    "net = weights_adj_all[0]\n",
    "for weights in weights_adj_all[1:]:\n",
    "    net = net @ weights\n",
    "net_df = pd.DataFrame(net, columns=gene_names, index=gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_df = pd.read_csv(\"../output/benchmark/grn_models/scenicplus.csv\")\n",
    "# net_df = net_df.pivot(index='source', columns='target', values='weight').fillna(0)\n",
    "# net_df = net_df.abs()\n",
    "# net_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n links: 4370736\n",
      "top_n=5, aucroc=0.5082225495511123, precision_mean=0.006695495302780784, matt_mean=0.0005570626029492135, f1_mean=0.001858002846117856\n",
      "n links: 4370736\n",
      "top_n=10, aucroc=0.5082225495511123, precision_mean=0.006695495302780784, matt_mean=0.0005570626029492135, f1_mean=0.001858002846117856\n",
      "n links: 4370736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[452], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39my_pred\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# metrics\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m aucroc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# precision, recall, tt = precision_recall_curve(y_true, y_pred)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# aucpr = (precision * recall).mean()\u001b[39;00m\n\u001b[1;32m     45\u001b[0m precision_mean \u001b[38;5;241m=\u001b[39m average_precision_score(y_true, y_pred)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:617\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    409\u001b[0m     {\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    429\u001b[0m ):\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    from prediction scores.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    619\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py:395\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(first_row):\n\u001b[1;32m    394\u001b[0m     first_row \u001b[38;5;241m=\u001b[39m first_row\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_array_api.py:307\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, matthews_corrcoef, average_precision_score\n",
    "\n",
    "def metric_curve(y_true, y_scores):\n",
    "    tt = np.linspace(0, y_scores.max(), 20)\n",
    "    matt_scores = []\n",
    "    f1_scores = []\n",
    "    # print(tt)\n",
    "    for t in tt[:-1]:\n",
    "        y_pred = y_scores>t\n",
    "        f1_scores.append(f1_score(y_true, y_pred))\n",
    "        matt_scores.append(matthews_corrcoef(y_true, y_pred))\n",
    "    \n",
    "    return  matt_scores, f1_scores\n",
    "\n",
    "for top_n in [5, 10, 100]:\n",
    "    # choose the top n genes with highest regulators\n",
    "    n_regulators = GT.sum(axis=0)\n",
    "    # GT_subset = GT[n_regulators.nlargest(top_n).index]\n",
    "    GT_subset = GT.copy()\n",
    "\n",
    "    # only those tfs and targets given in GT\n",
    "    def match_dfs(df1, df2):\n",
    "        df1 = df1.loc[df1.index.isin(df2.index), df1.columns.isin(df2.columns)]\n",
    "        df2 = df2.loc[df2.index.isin(df1.index), df2.columns.isin(df1.columns)]\n",
    "        return df1, df2\n",
    "    net_subset, GT_subset = match_dfs(net_df, GT_subset)\n",
    "    # reindex for compatibility\n",
    "    net_subset = net_subset.reindex(index=GT_subset.index, columns=GT_subset.columns)\n",
    "\n",
    "    y_pred = net_subset.values.flatten()\n",
    "    y_true = GT_subset.values.flatten()\n",
    "\n",
    "    # no negative reg\n",
    "    y_pred = np.abs(y_pred)\n",
    "\n",
    "    print('n links:',len(y_pred))\n",
    "\n",
    "    assert np.isnan(y_pred).any()==False\n",
    "    assert y_true.shape==y_pred.shape\n",
    "\n",
    "    # metrics\n",
    "    aucroc = roc_auc_score(y_true, y_pred)\n",
    "    # precision, recall, tt = precision_recall_curve(y_true, y_pred)\n",
    "    # aucpr = (precision * recall).mean()\n",
    "    precision_mean = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    matt_scores, f1_scores = metric_curve(y_true, y_pred)\n",
    "    matt_mean = np.mean(matt_scores)\n",
    "    f1_mean = np.mean(f1_scores)\n",
    "\n",
    "    print(f'{top_n=}, {aucroc=}, {precision_mean=}, {matt_mean=}, {f1_mean=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n links: 303\n",
    "top_n=5, aucroc=0.5264077669902912, precision_mean=0.3669703794235179, matt_mean=0.10818906873658679, f1_mean=0.08888404098938463\n",
    "n links: 707\n",
    "top_n=10, aucroc=0.5187080867850099,  precision_mean=0.3046555354494385, matt_mean=0.09383949611263599, f1_mean=0.07612230579002001\n",
    "n links: 7272\n",
    "top_n=100, aucroc=0.5232494638773373, precision_mean=0.1546841109780841, matt_mean=0.059091840774979656, f1_mean=0.036139223144212335\n",
    "n links: 74235\n",
    "top_n=1000, aucroc=0.532563048262141, precision_mean=0.05838751393542791, matt_mean=0.06267851369070392, f1_mean=0.042948548762372384\n",
    "n links: 370670\n",
    "top_n=5000, aucroc=0.5308883277329034, precision_mean=0.021350126982468423, matt_mean=0.045122637323227084, f1_mean=0.028030001732845523\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maa\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import plots\n",
    "plots.plot_cumulative_density(reg_net.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency map approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NN explainer\n",
    "# baseline = torch.zeros((1, X.size()[1], X.size()[2]))\n",
    "# if USE_GPU:\n",
    "#     baseline = baseline.cuda()\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Sort the columns of the goldstandard network based on the coverage of annotations\n",
    "idx = np.argsort(np.sum(grn, axis=0))[::-1]\n",
    "\n",
    "all_z_target, all_z_pred = [], []\n",
    "for k in range(5):\n",
    "    \n",
    "    # Select column `j` from the goldstandard GRN\n",
    "    j = int(idx[k])\n",
    "    z_target = grn[:, j]\n",
    "    z_target[j] = np.nan  # Assume that a gene cannot regulate itself -> will be filtered out later\n",
    "    all_z_target.append(z_target)\n",
    "    \n",
    "    # Compute feature attribution vectors for many input points and average over the results\n",
    "    z_pred = np.zeros(len(gene_names))\n",
    "    for i in tqdm.tqdm(range(1000)):\n",
    "        # Generate random input based on the null distribution\n",
    "        input_ = background_noise(X.size()[1], X.size()[2], device=baseline.device)\n",
    "        \n",
    "        # Compute feature attribution\n",
    "        attributions = ig.attribute(input_, baseline, target=j, return_convergence_delta=False)\n",
    "        # An alternative is to use input samples from the training set:\n",
    "        # attributions = ig.attribute(X[i, ...], baseline, target=j, return_convergence_delta=False)\n",
    "        # By averaging across all samples from the same cell type, one can build a biological network\n",
    "        # that is cell type-specific.\n",
    "        \n",
    "        z_pred += np.mean(np.abs(np.squeeze(attributions.cpu().data.numpy())), axis=1)\n",
    "    all_z_pred.append(z_pred[grn_mask])\n",
    "    \n",
    "# Compute performance metrics\n",
    "all_z_target = np.concatenate(all_z_target, axis=0)\n",
    "all_z_pred = np.concatenate(all_z_pred, axis=0)\n",
    "mask = ~np.isnan(all_z_target)  # Discard self-regulations\n",
    "print(f'AUROC: {roc_auc_score(all_z_target[mask], all_z_pred[mask])}')\n",
    "print(f'AUPR : {average_precision_score(all_z_target[mask], all_z_pred[mask])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # predict\n",
    "# model.eval()\n",
    "# y_pred_submit, mu, log_var = model(torch.tensor(X_submit, dtype=torch.float32, device='cuda'))\n",
    "# y_pred_submit = y_pred_submit.cpu().detach().numpy()\n",
    "# y_pred_submit_all.append(y_pred_submit)\n",
    "\n",
    "# print('r2:', r2_score(df_test, y_pred_submit, multioutput='variance_weighted'))\n",
    "# models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T16:28:02.331255Z",
     "iopub.status.busy": "2024-06-03T16:28:02.330801Z",
     "iopub.status.idle": "2024-06-03T16:28:02.342089Z",
     "shell.execute_reply": "2024-06-03T16:28:02.341161Z",
     "shell.execute_reply.started": "2024-06-03T16:28:02.331213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-03T15:09:34.795825Z",
     "iopub.status.idle": "2024-06-03T15:09:34.801895Z",
     "shell.execute_reply": "2024-06-03T15:09:34.801613Z",
     "shell.execute_reply.started": "2024-06-03T15:09:34.801578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('number of models:', len(y_pred_submit_all))\n",
    "y_pred_submit_mean = np.mean(y_pred_submit_all, axis=0)\n",
    "print('r2:', r2_score(df_test, y_pred_submit_mean, multioutput='variance_weighted'))\n",
    "print('mse:', mean_squared_error(y_pred_submit_mean, df_test.values))\n",
    "print('mrrmse:', mrrmse(y_pred_submit_mean, df_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-03T15:09:34.809004Z",
     "iopub.status.idle": "2024-06-03T15:09:34.813749Z",
     "shell.execute_reply": "2024-06-03T15:09:34.813459Z",
     "shell.execute_reply.started": "2024-06-03T15:09:34.813421Z"
    }
   },
   "outputs": [],
   "source": [
    "r2: 0.24227989212421114\n",
    "mse: 1.7873100056454099\n",
    "mrrmse: 0.8062594076816958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-03T15:09:34.830931Z",
     "iopub.status.idle": "2024-06-03T15:09:34.835443Z",
     "shell.execute_reply": "2024-06-03T15:09:34.835119Z",
     "shell.execute_reply.started": "2024-06-03T15:09:34.835076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(df_train, df_test, feature_space):\n",
    "    # encode each factor\n",
    "    x_encoded_dict = {}\n",
    "    for feature in feature_space:\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        n_com = min([len(index.unique()), 50])\n",
    "        # var_x = TruncatedSVD(n_components=n_com, n_iter=12, random_state=random_state).fit_transform(df_train)\n",
    "        var_x = KernelPCA(n_components=n_com, kernel='linear', random_state=random_state).fit_transform(df_train)\n",
    "        x_encoded = pd.DataFrame(var_x, index=index).reset_index()\n",
    "        x_encoded = x_encoded.groupby(feature).mean()\n",
    "        x_encoded_dict[feature] = x_encoded\n",
    "    # create X and X_submit\n",
    "    X = []\n",
    "    X_submit = []\n",
    "    for i_feature, feature in enumerate(feature_space):\n",
    "        # encode train data\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X = feature_encoded\n",
    "        else: \n",
    "            X = np.concatenate([X, feature_encoded], axis=1)\n",
    "        \n",
    "        # encode test data\n",
    "        index = df_test.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X_submit = feature_encoded\n",
    "        else: \n",
    "            X_submit = np.concatenate([X_submit, feature_encoded], axis=1)\n",
    "    return X, X_submit\n",
    "\n",
    "\n",
    "random_state = 32\n",
    "n_components = 50\n",
    "X_rf, X_submit_rf = encode(df_train, df_test, features_X)\n",
    "emb_model = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "reducer = TruncatedSVD(n_components=n_components, n_iter=12, random_state=random_state)\n",
    "Y = reducer.fit_transform(df_train)\n",
    "\n",
    "emb_model.fit(X_rf, Y)\n",
    "y_pred_submit = reducer.inverse_transform(emb_model.predict(X_submit_rf))\n",
    "\n",
    "print('r2:', r2_score(df_test, y_pred_submit, multioutput='variance_weighted'))\n",
    "print('mse:', mean_squared_error(y_pred_submit, df_test.values))\n",
    "print('mrrmse:', mrrmse(y_pred_submit, df_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-03T15:09:34.839314Z",
     "iopub.status.idle": "2024-06-03T15:09:34.844806Z",
     "shell.execute_reply": "2024-06-03T15:09:34.844512Z",
     "shell.execute_reply.started": "2024-06-03T15:09:34.844480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2: 0.24227989212421114\n",
    "mse: 1.7873100056454099\n",
    "mrrmse: 0.8062594076816958"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
